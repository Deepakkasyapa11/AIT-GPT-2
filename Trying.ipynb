{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abd2299-6fe1-4ab4-be4b-0ba5680b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.handlers as handlers\n",
    "import sys\n",
    "import os\n",
    "def init_logger(name:str, filename:str, path:str='./log', level:int=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Check path exist\n",
    "    if(os.path.exists(path) == False):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    file = os.path.join(path,f\"{filename}.log\")\n",
    "    formatter = logging.Formatter('%(asctime)s|%(filename)s:%(lineno)d|%(levelname)s|%(message)s')\n",
    "    formatter.datefmt = '%d-%m-%Y %H:%M:%S'\n",
    "    # Handler\n",
    "    consoleHandler = logging.StreamHandler(sys.stdout)\n",
    "    consoleHandler.setFormatter(formatter)\n",
    "    # This will rotate log\n",
    "    fileHandler = handlers.RotatingFileHandler(filename=file, mode='a', maxBytes=10240000, backupCount=10)\n",
    "    fileHandler.setFormatter(formatter)\n",
    "\n",
    "    # Add Handler\n",
    "    logger.addHandler(consoleHandler)\n",
    "    logger.addHandler(fileHandler)\n",
    "\n",
    "    logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3655c630-39b7-4573-96a2-4a7e43b90299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "# from components.logger import init_logger\n",
    "# from components.model import load_model \n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains.retrieval_qa.base import BaseRetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "\n",
    "# A bunch of global variable\n",
    "MODEL_NAME:str = \"mlflow-example\"\n",
    "STAGE:str = \"Production\"\n",
    "MLFLOW_URL:str = \"http://la.cs.ait.ac.th\"\n",
    "CACHE_FOLDER:str = os.path.join(\"/root\",\"cache\")\n",
    "embedding_model:HuggingFaceInstructEmbeddings\n",
    "vector_database:FAISS\n",
    "llm_model:LLM\n",
    "qa_retriever:BaseRetrievalQA\n",
    "conversational_qa_memory_retriever:ConversationalRetrievalChain\n",
    "question_generator:LLMChain\n",
    "\n",
    "device=\"cuda:1\"\n",
    "device_id=1\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are the chatbot and the face of Asian Institute of Technology (AIT). Your job is to give answers to prospective and current students about the school.\n",
    "Your job is to answer questions only and only related to the AIT. Anything unrelated should be responded with the fact that your main job is solely to provide assistance regarding AIT.\n",
    "MUST only use the following pieces of context to answer the question at the end. If the answers are not in the context or you are not sure of the answer, just say that you don't know, don't try to make up an answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "When encountering abusive, offensive, or harmful language, such as fuck, bitch,etc,  just politely ask the users to maintain appropriate behaviours.\n",
    "Always make sure to elaborate your response and use vibrant, positive tone to represent good branding of the school.\n",
    "Never answer with any unfinished response\n",
    "Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "def load_scraped_web_info():\n",
    "    with open(\"ait-web-document\", \"rb\") as fp:\n",
    "        ait_web_documents = pickle.load(fp)\n",
    "        \n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap  = 100,\n",
    "        length_function = len,\n",
    "    )\n",
    "\n",
    "    chunked_text = text_splitter.create_documents([doc for doc in tqdm(ait_web_documents)])\n",
    "\n",
    "def load_embedding_model():\n",
    "    embedding_model = HuggingFaceInstructEmbeddings(model_name='hkunlp/instructor-base',\n",
    "                                                    cache_folder='./.cache',\n",
    "                                                model_kwargs = {'device': torch.device(device)}\n",
    "                                                )\n",
    "    return embedding_model\n",
    "\n",
    "def load_faiss_index():\n",
    "    global embedding_model\n",
    "    vector_database = FAISS.load_local(\"faiss_index_web_and_curri_new\", embedding_model) #CHANGE THIS FAISS EMBEDDED KNOWLEDGE\n",
    "    return vector_database\n",
    "\n",
    "def load_llm_model_cpu():\n",
    "    llm = HuggingFacePipeline.from_model_id(model_id= 'lmsys/fastchat-t5-3b-v1.0', \n",
    "                            task= 'text2text-generation',        \n",
    "                            model_kwargs={ \"max_length\": 256, \"temperature\": 0,\n",
    "                                            \"torch_dtype\":torch.float32,\n",
    "                                        \"repetition_penalty\": 1.3})\n",
    "\n",
    "    return llm\n",
    "\n",
    "def load_llm_model_gpu(gpu_id:int ):\n",
    "    llm = HuggingFacePipeline.from_model_id(model_id= 'lmsys/fastchat-t5-3b-v1.0', \n",
    "                                            task= 'text2text-generation',\n",
    "                                            device=device_id,\n",
    "                                            model_kwargs={ \n",
    "                                                # \"device_map\": \"auto\",\n",
    "                                                        # \"load_in_8bit\": True,\n",
    "                                                        \"max_length\": 256, \n",
    "                                                        \"offload_folder\": \"offload\",\n",
    "                                                        \"temperature\": 0,\n",
    "                                                        \"repetition_penalty\": 1.5},\n",
    "                                            )\n",
    "\n",
    "    return llm\n",
    "\n",
    "def load_conversational_qa_memory_retriever():\n",
    "    global vector_database, llm_model\n",
    "\n",
    "    question_generator = LLMChain(llm=llm_model, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "    doc_chain = load_qa_chain(llm_model, chain_type=\"stuff\", prompt = PROMPT)\n",
    "    memory = ConversationBufferWindowMemory(k = 3,  memory_key=\"chat_history\", return_messages=True,  output_key='answer')\n",
    "    \n",
    "    \n",
    "    \n",
    "    conversational_qa_memory_retriever = ConversationalRetrievalChain(\n",
    "        retriever=vector_database.as_retriever(),\n",
    "        question_generator=question_generator,\n",
    "        combine_docs_chain=doc_chain,\n",
    "        return_source_documents=True,\n",
    "        memory = memory,\n",
    "        get_chat_history=lambda h :h)\n",
    "    return conversational_qa_memory_retriever, question_generator\n",
    "\n",
    "def load_retriever(llm, db):\n",
    "    qa_retriever = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
    "                            retriever=db.as_retriever(),\n",
    "                            chain_type_kwargs= chain_type_kwargs)\n",
    "\n",
    "    return qa_retriever\n",
    "\n",
    "def retrieve_document(query_input):\n",
    "    global vector_database\n",
    "    related_doc = vector_database.similarity_search(query_input)\n",
    "    return related_doc\n",
    "\n",
    "def retrieve_answer(my_text_input:str):\n",
    "    global qa_retriever\n",
    "    prompt_answer=  my_text_input\n",
    "    answer = qa_retriever.run(prompt_answer)\n",
    "    log = {\"timestamp\": datetime.datetime.now(),\n",
    "        \"question\":my_text_input,\n",
    "        \"generated_answer\": answer[6:],\n",
    "        \"rating\":0 }\n",
    "\n",
    "    # TODO: change below code and maintain in session\n",
    "    # st.session_state.history.append(log)\n",
    "    # update_worksheet_qa()\n",
    "    # st.session_state.chat_history.append({\"message\": st.session_state.my_text_input, \"is_user\": True})\n",
    "    # st.session_state.chat_history.append({\"message\": answer[6:] , \"is_user\": False})\n",
    "\n",
    "    # st.session_state.my_text_input = \"\"\n",
    "\n",
    "    return answer[6:] #this positional slicing helps remove \"<pad> \" at the beginning\n",
    "\n",
    "def main():\n",
    "    global embedding_model, vector_database, llm_model, qa_retriever, conversational_qa_memory_retriever, question_generator\n",
    "    # Init Logger\n",
    "    init_logger(name=\"main\", filename=\"main\", path=\"./logs/\", level=logging.DEBUG)\n",
    "    logger = logging.getLogger(name=\"main\")\n",
    "    # Now prepare model\n",
    "\n",
    "    # MODEL = load_model(model_name=MODEL_NAME, stage=STAGE, cache_folder=CACHE_FOLDER)\n",
    "    load_scraped_web_info()\n",
    "    embedding_model = load_embedding_model()\n",
    "    vector_database = load_faiss_index()\n",
    "    # llm_model = load_llm_model_cpu()\n",
    "    llm_model = load_llm_model_gpu(0)\n",
    "    qa_retriever = load_retriever(llm= llm_model, db= vector_database)\n",
    "    conversational_qa_memory_retriever, question_generator = load_conversational_qa_memory_retriever()\n",
    "\n",
    "    return 0\n",
    "\n",
    "def get_root(text: str):\n",
    "    timestamp = datetime.datetime.now()\n",
    "    ans = retrieve_answer(text)\n",
    "    timestamp2 = datetime.datetime.now()\n",
    "    delta = timestamp2 - timestamp\n",
    "    print(f\"Time taken is {delta.total_seconds()} seconds\")\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca2fb59-8b4f-4e6c-9fd3-f3aad47398ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 205/205 [00:00<00:00, 4478293.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a82680-05e3-4340-abe1-9f3a6b960ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 2.542579 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The  Asian  Institute  of  Technology  (AIT)  is  an  international  English-speaking  postgraduate  institution,  focusing  on  engineering,  environment,  and  management  studies.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_root(\"What is AIT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915b3cc8-e64d-41da-a286-0647ef02ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken is 6.198089 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'If  you  are  taking  computer  science  as  your  major,  the  subjects  that  you  need  to  take  include:\\n *  Calculus\\n *  Discrete  Mathematics\\n *  Linear  Algebra\\n *  Basic  Computer  Programming\\n *  Data  Modeling  and  Management\\n *  Business  Intelligence  and  Analytics\\n The  core  curriculum  in  computer  science  covers  all  aspects  of  computing,  with  the  faculty  particularly  active  in  artificial  intelligence,  software  engineering,  networking,  and  information  systems.  Students  are  also  encouraged  to  take  courses  and  conduct  research  in  areas  of  computer  science  that  interact  with  Information  Management,  Industrial  Engineering,  Manufacturing  Systems  Engineering,  Telecommunications,  Mechatronics,  and  other  fields  covered  at  the  Institute.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_root(\"what are the subjects that I need to take if I am taking computer science as my major\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ea9b4-fb87-44f3-8fe7-29d4d05721ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
